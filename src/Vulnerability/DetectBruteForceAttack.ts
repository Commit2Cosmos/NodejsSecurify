import { BayesClassifier, WordTokenizer } from 'natural';
import { parseCSV } from '../parsingCSVData';
import { readFileSync } from "node:fs"

interface DatasetSample {
    code: string;
    label: number;
}

// removing repeated data
function removeRedundantData(dataset: DatasetSample[]): DatasetSample[] {
    const uniqueEntries: DatasetSample[] = [];
    const seenEntries: Set<string> = new Set();
    let count0: number = 0;
    let count1: number = 0;
    for (const entry of dataset) {
        const entryString = JSON.stringify(entry);
        if (!seenEntries.has(entryString)) {
            if (entry.label === 0) count0++;
            else count1++;
            uniqueEntries.push(entry);
            seenEntries.add(entryString);
        }
    }
    // console.log(count0, count1, uniqueEntries.length);
    return uniqueEntries;
}

// Function to detect vulnerability brute force attack in a new code snippet
export function detectBruteForce(code_snippet: string): void {
    let detect: boolean = false;
    // Create a tokenizer
    const tokenizer = new WordTokenizer();

    // Vectorize the new code snippet
    const tokenizedSnippet = tokenizer.tokenize(code_snippet);

    // Parsing dataset from CSV   
    const csv_data = readFileSync("src/datasets/bruteForceDataset.csv", { encoding: "utf8" });
    const dataset = parseCSV(csv_data)
        
    // Make prediction using the trained classifier
    if (tokenizedSnippet !== null) {
        // Prepare the data for training
        const cleanedDataset = removeRedundantData(dataset);
        const code_samples: string[] = cleanedDataset.map((sample) => sample.code);
        const labels: number[] = cleanedDataset.map((sample) => sample.label);
        // Vectorize the code samples using the tokenizer
        const tokenizedSamples: string[][] = code_samples
            .map((code) => tokenizer.tokenize(code))
            .filter((tokens) => tokens !== null) as string[][];
        // Train a Naive Bayes classifier
        const classifier = new BayesClassifier();
        for (let i = 0; i < tokenizedSamples.length; i++) {
            classifier.addDocument(tokenizedSamples[i], labels[i].toString());
        }
        classifier.train();
        const prediction = classifier.classify(tokenizedSnippet);
        const result = parseInt(prediction);
        if (result) {
            detect = true;
            const result: string = "==> Code vulnerable to Brute force Attack in this file!!! ";
            console.log(result.red);
        }
    }
    if (!detect) {
        console.log("==> Code NOT vulnerable to Brute force Attack".green);
    }
    
}
