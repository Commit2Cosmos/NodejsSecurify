import { BayesClassifier, WordTokenizer } from 'natural';

import { dataset as bruteForceDataset } from './DetectBruteForceAttack';
import { dataset as inputValidationDataset } from './DetectInputValidation';
import { dataset as insecureAuthenticationDataset } from './InsecureAuthentication';
import { dataset as securityHeadersDataset } from './AnalyzeSecurityHeaders';

import { DatasetSample } from './DatsetSample';
import { removeRedundantData } from './DatasetUtils';

// Enumerates common types of vulnerabilities.
// Note: Defined as a 'read-only' frozen object to prevent modifications.
export const Vulnerability = Object.freeze({
    BruteForce: 0,
    InputValidation: 1,
    InsecureAuth: 2,
    SecurityHeaders: 3,
} as const);

export type Vulnerability = (typeof Vulnerability)[keyof typeof Vulnerability];

// Map `Vulnerability` values to their `toString` descriptions.
export const toStringMap: { [key: number]: string } = Object.freeze({
    [Vulnerability.BruteForce]: 'Brute force Attack',
    [Vulnerability.InputValidation]: 'input validation',
    [Vulnerability.InsecureAuth]: 'Insecure Authentication',
    [Vulnerability.SecurityHeaders]: 'Security Headers',
} as const);

// Map `Vulnerability` values to their dataset samples.
export const toDatasetMap: { [key: number]: readonly DatasetSample[] } = Object.freeze({
    [Vulnerability.BruteForce]: bruteForceDataset,
    [Vulnerability.InputValidation]: inputValidationDataset,
    [Vulnerability.InsecureAuth]: insecureAuthenticationDataset,
    [Vulnerability.SecurityHeaders]: securityHeadersDataset,
} as const);

export const notDetectedToStringMap: { [key: number]: string } = Object.freeze({
    [Vulnerability.BruteForce]: `==> Code NOT vulnerable to ${toStringMap[Vulnerability.BruteForce]}`,
    [Vulnerability.InputValidation]: `==> Code has proper ${toStringMap[Vulnerability.InputValidation]}`,
    [Vulnerability.InsecureAuth]: `==> NO ${toStringMap[Vulnerability.InsecureAuth]} detected`,
    [Vulnerability.SecurityHeaders]: `==> No Insecure ${toStringMap[Vulnerability.SecurityHeaders]} found`, // ok: security headers safe from brute force attacks
} as const);

export const detectedToStringMap: { [key: number]: string } = Object.freeze({
    [Vulnerability.BruteForce]: `==> Code vulnerable to ${toStringMap[Vulnerability.BruteForce]} in this file!!! `,
    [Vulnerability.InputValidation]: `==> Code has improper ${toStringMap[Vulnerability.InputValidation]}, vulnerable to DOS Attack:!!! `,
    [Vulnerability.InsecureAuth]:
        `==> Code vulnerable to ${toStringMap[Vulnerability.InsecureAuth]} in this file!!! ` +
        '\nCheck for: Hardcoded Credentials, Weak Passwords, No Password Hashing, Insecure Session Management, Insecure Token Storage, Weak JWT Security',
    [Vulnerability.SecurityHeaders]: `==> Insecure ${toStringMap[Vulnerability.SecurityHeaders]} found!!! `, // not ok: security headers NOT safe from brute force attacks
} as const);

export function detectVulnerability(kind: Vulnerability, codeSnippet: string): void {
    let isDetected: boolean = false;
    // Create a tokenizer.
    const tokenizer = new WordTokenizer();
    const tokenizedSnippet = tokenizer.tokenize(codeSnippet) as readonly string[] | null;

    // Make prediction using the trained classifier.
    if (tokenizedSnippet !== null) {
        // Prepare the data for training.
        const dataset: readonly DatasetSample[] = toDatasetMap[kind];
        const cleanedDataset: readonly DatasetSample[] = removeRedundantData(dataset);

        const codeSamples: readonly string[] = cleanedDataset.map((sample) => sample.code);
        const labels: readonly number[] = cleanedDataset.map((sample) => sample.label);

        // Vectorize the code samples using the tokenizer.
        const tokenizerSamples = codeSamples
            .map((code) => tokenizer.tokenize(code))
            .filter((tokens): tokens is string[] => tokens !== null) as (readonly string[])[];

        // Train a Naive Bayes classifier.
        const classifier = new BayesClassifier();
        for (let i = 0; i < tokenizerSamples.length; i++) {
            classifier.addDocument([...tokenizerSamples[i]], labels[i].toString()); // copy ensures immutability
        }
        classifier.train();
        const prediction: string = classifier.classify([...tokenizedSnippet]);
        const result: number = parseInt(prediction, 10);
        if (result === 1) {
            isDetected = true;
            console.log(detectedToStringMap[kind].red);
        }
    }

    if (!isDetected) {
        console.log(notDetectedToStringMap[kind].green);
    }
}

// TEST

const __isSkipTest = true; // WARN: << DEVELOPMENT ONLY >>

import { testVulnerability } from './Vulnerability_test';

if (!__isSkipTest) {
    const timerLabel = `[test]: ${testVulnerability.name}`;
    {
        console.time(timerLabel);
        (async () => await testVulnerability())();
        console.timeEnd(timerLabel);
    }
}